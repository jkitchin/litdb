#+title: litdb - a literature and document database

* litdb concept

litdb is a tool to help you curate and us your collection of scientific literature. You use it to collect and search papers. You can use it to collect older articles, and to keep up with newer articles. litdb uses https://openalex.org for searching the scientific literature, and https://turso.tech/libsql to store results in a local database.

The idea is you add papers to your database, and then you can search it with natural language queries, and interact with it via an ollama GPT application. It will show you the papers that best match your query. You can read those papers, get bibtex entries for them, or add new papers based on the references, papers that cite that paper, or related papers. You can also set up filters that you update when you want to get new papers created since the last time you checked.

** configuration

You have to create a toml configuration file. This file is called litdb.toml. The directory this file is in is considered the root directory. All commands will start in the current working directory and look up to find this file. You can put this file in your home directory, or you can have sub-directories, e.g. a per project litdb.

There are a few choices you have to make. You have to choose a SentenceTransformer model, and specify the size of the vectors it makes. You also have to specify the chunk_size and chunk_overlap settings that are used to break documents up to compute document level embedding vectors. 

You will need an OpenAlex premium key if you want to use the update-filters feature.

#+BEGIN_EXAMPLE
[database]
# relative to root directory
db = "litdb.libsql"

[embedding]
model = 'all-MiniLM-L6-v2'
embedding_size = 384
chunk_size = 1000
chunk_overlap = 200


[openalex]
email = "you@example.com"
api_key = "..."
#+END_EXAMPLE


* How do you grow your litdb?

Your litdb starts out empty. You have to add articles that are relevant to you. It is an open question of the best way to build a litdb. The answer surely depends on what your aim is. You have to compromise on breadth and depth with the database size. Here are a few examples.

Each of these approaches could be integrated with daily updates to add new articles that meet the filter criteria.

- seed it with articles, and pull their references, citing and related papers. You choose the articles so they are automatically relevant. 

- Seed it with an author. Say you know an author who does work related to your interest. You can just get all their work to start it. This obviously biases your litdb to people you know.

- Import from a bibtex file. If you already have a bibtex file, you might use it to add all the articles to your litdb. This works on DOI, so only items with a doi are added.

- Seed it with a query. This would be a keyword or full text search query. These can be large. For example, a search on catalysis articles with publication dates greater than 2019 yields 40K results. This is likely to exceed the libsql could db size allowed (9GB). I am not sure how small I can make the db, right now it stores the text. Maybe I can just store the work id and embeddings, and look up data at search time? The query could also include journals if you think it makes sense. 

- Update your queries regularly. You can automate this, or do it manually. This will add new entries since the last update.

** The CLI

litdb has a cli with an entry command of litdb and subcommands (like git) for interacting with it. You can see all the options with this command.

#+BEGIN_SRC sh :dir example
litdb --help
#+END_SRC

*** One time additions of articles to litdb

You add an article by its DOI. There are optional arguments to also add references, citing and related articles. 

#+BEGIN_SRC sh
litdb add doi --references --citing --related
#+END_SRC

To add an author, use their orcid. You can use ~litdb author-search firstname lastname~ to find an orcid for a person.

#+BEGIN_SRC sh
litdb add orcid
#+END_SRC

To add entries from a bibtex file, use the path to the file.

#+BEGIN_SRC sh
litdb add /path/to/bibtex.bib
#+END_SRC

You can provide more than one source and even mix them like this.

#+BEGIN_SRC sh
litdb add doi1 doi2 orcid
#+END_SRC


These are all one time additions.

*** Adding filters

litdb provides several convenient ways to add queries to update your litdb in the future.

**** Follow an author

To get new papers by an author, you can follow them.

#+BEGIN_SRC sh
litdb follow orcid
#+END_SRC

**** Watch a query

#+BEGIN_SRC sh
litdb watch "filter to query"
#+END_SRC

**** Citations on a paper

#+BEGIN_SRC sh
litdb citing doi
#+END_SRC

**** Related papers

#+BEGIN_SRC sh
litdb related doi
#+END_SRC

**** A custom filter

A filter is used in OpenAlex to search for relevant articles. Here is an example of adding a filter for articles in the journal Digital Discovery. This doesn't add any entries directly, it simply stores the filter in the database. The main difference of this vs the watch command above is the explicit description.

#+BEGIN_SRC sh
litdb add-filter "primary_location.source.id:https://openalex.org/S4210202120" -d "Digital Discovery"
#+END_SRC

You can remove a filter like this:

#+BEGIN_SRC sh
litdb rm-filter "filter-string"
#+END_SRC

**** Managing and updating the filters

You can get a list of your filters like this.


#+BEGIN_SRC sh
litdb list-filters
#+END_SRC

You can update the filters like this.

#+BEGIN_SRC sh
litdb update-filters
#+END_SRC

This adds papers that have been created since the last time you ran the filter. You need an OpenAlex premium API key for this. This will update the last_updated field.

*** Searching litdb

There are three search options. 

**** Vector search

The main way litdb was designed to be searched is with by natural language queries. The way this works is your query is converted to a vector using SentenceTransformers, and then a vector search identifies entries in the database that are similar to your query.

#+BEGIN_SRC sh
litdb vsearch "natural language query" 
#+END_SRC

The default number of entries returned is 3. You can change that with an optional argument

#+BEGIN_SRC sh
litdb vsearch "natural language query" -n 5
#+END_SRC

**** full text search

There is a full text search (full on the text in litdb) available. The command looks like this.

#+BEGIN_SRC sh
litdb fulltext "query"
#+END_SRC

See https://sqlite.org/fts5.html for information on what the query might look like. The search is done with this SQL command:

#+BEGIN_SRC sql
select source, text from fulltext where text match ? order by rank
#+END_SRC

The default number of entries returned is 3. You can change that with an optional argument

#+BEGIN_SRC sh
litdb fulltext "natural language query" -n 5
#+END_SRC

**** ollama GPT

You can use litdb as a RAG source for ollama. This looks up the three most related papers to your query, and uses them as context in a prompt to ollama (with the llama2 model). I find this quite slow (it can be minutes to generate a response on an old Intel Mac). I also find it makes up things like references, and that it is usually necessary to actually read the three papers. The three papers come from the same vector search described above.

#+BEGIN_SRC sh
litdb gpt "what is the state of the art in automated laboratories for soft materials"
#+END_SRC

*** Exporting entries

You can use these commands to export bibtex entries or citation strings.

**** Get a bibtex entry

This command will try to generate a bibtex entry for entries in your litdb.

#+BEGIN_SRC sh
litdb bibtex doi1 doi2
#+END_SRC

The output can be redirected to a file.

**** Get a citation string

#+BEGIN_SRC sh
litdb citation doi1 doi2
#+END_SRC

*** Low-level interaction with litdb

litdb is just a sqlite database (although you need to use the libsql executable for vector search). There is a CLI way to run a sql command. For example, to find all entries with a null bibtex field and their types use a query like this.

#+BEGIN_SRC sh
litdb sql "select source, json_extract(extra, '$.type'), json_extract(extra, '$.bibtex') as bt from sources where bt is null"
#+END_SRC

You might also use this for very specific queries. For example, here I search the citation strings for my name.

#+BEGIN_SRC sh
litdb sql "select source, json_extract(extra, '$.citation') as citation from sources where citation like '%kitchin%'"
#+END_SRC


* Database design

litdb uses a sqlite database with libsql. libsql is a sqlite fork with additional capabilities, most notably integrated vector search. 

The main table in litdb is called sources.
- sources
  - source (url to source location)
  - text (the text for the source)
  - extra (json data)
  - embedding (float32 blob in bytes)
  - date_added string

This table has an embedding_idx index for vector search.

There is also a virtual table fulltext for fulltext search.

- fulltext
  - source
  - text

And a table called queries.
- queries
  - filter
  - description
  - last_updated

This database is automatically created when you use litdb.

* Limitations

The text that is stored for each entry comes from OpenAlex and is typically limited to the title and abstract. For the text in each entry The first line is typically a citation including the title, and the rest is the abstract if there is one.

The quality of the vector search depends on several things. First, litdb stores a document level embedding vector that is computed by averaging the embedding vectors of overlapping chunks. We use Sentence Transformers to compute these. There are many choices to make on the model, and these have not been tested exhaustively. So far 'all-MiniLM-L6-v2' works well enough. There are other models you could consider like getting embeddings from ollama, but at the moment litdb can only use SentenceTransformers.

I guess that document level embeddings are less effective on longer documents. The title+abstract from OpenAlex is pretty short, and so far there isn't evidence this is a problem.

Second, we rely on defaults in libsql for the vector search, notably finding the top k nearest vectors based on cosine similarity. There are other distance metrics you could use like L2, but we have not considered these.

Finally, the query is based on vector similarity between your query and the texts. So, you should write the query so it looks like what you want to find, rather than as a question.

* Roadmap

These are ideas for future expansion.

** Adding local files

It is possible to add any file that can be turned into text to litdb. That includes:

- pdf
- docx
- pptx
- html
- org / md

This limits portability because you need a path if you want to be able to open that file.

** format specifications

There is no way to specify the format of outputs. Eventually that might be nice, e.g. specify to only output paths, so you can pipe those to another command.

** Emacs integration

Eventually I anticipate Emacs being the main UI to litdb. I don't know if it will replace org-db or augment it. 

** Review process 

You need to have a way to review what comes in to litdb; it is part of learning about what is current. I currently do this with Emacs 

** web app / fast-api

It might be nice to have a flask app with an API. This would facilitate interaction with Emacs.

** tag system

It could be useful to have a tag system where you could label entries, or they could be auto-tagged when updating filters. This would allow you to tag entries by a project, or select entries for some kind of bulk action like update, export to bibtex, or delete.

You might also build a scoring system, e.g. for like/dislike tags.

** graph visualization

It might be helpful to have a graph representation of a paper that shows nodes of citing, references, and related papers, with edge length related to a similarity score, and node size related to number of citations.

** Semantic similarity 

litdb uses cosine similarity as the distance metric for the nearest neighbors. It might be useful to re-rank these with cross-encoding.

https://www.sbert.net/examples/applications/cross-encoder/README.html

** Combine full text and vector search

Vector search might miss some things. Full text search is hard to do with meaning. There are several ways to do a hybrid search, e.g. do a full text search on keywords, and a vector search, and use some kind of union on those results.

https://www.meilisearch.com/blog/full-text-search-vs-vector-search

** Application specific encoders

I use a generic embedding model, and there are others that are better suited for specific tasks. For example:

- MatBERT [[cite:&trewartha-2022-quant-advan]]
- Scibert [[cite:&beltagy-2019-sciber]]
- Matscibert [[cite:&gupta-2022-matsc]]
- Specter cite:&cohan-2020-spect https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#scientific-similarity-models
- PaECTER [[cite:&ghosh-2024-paect]] for patents

litdb add https://doi.org/10.48550/arXiv.2004.07180 --related --citing --references

these might have a variety of uses with litdb that range from extracting data, named entity recognition, specific searches on materials, etc.

** Image and text models

One day it might be possible to include images in this (https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#image-text-models). At the moment, OpenAlex entries do not have any images, but local files would. I have an image database in org-db, but I don't use it a lot. 

